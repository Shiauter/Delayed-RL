{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10b4f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.utils import probs_to_logits\n",
    "import torch.multiprocessing as mp\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Categorical:\n",
    "    def __init__(self, probs_shape):\n",
    "        # NOTE: probs_shape is supposed to be\n",
    "        #       the shape of probs that will be\n",
    "        #       produced by policy network\n",
    "        if len(probs_shape) < 1:\n",
    "            raise ValueError(\"`probs_shape` must be at least 1.\")\n",
    "        self.probs_dim = len(probs_shape)\n",
    "        self.probs_shape = probs_shape\n",
    "        self._num_events = probs_shape[-1]\n",
    "        self._batch_shape = probs_shape[:-1] if self.probs_dim > 1 else torch.Size()\n",
    "        self._event_shape=torch.Size()\n",
    "\n",
    "    def set_probs(self, probs):\n",
    "        # normalized the probs\n",
    "        self.probs = probs / probs.sum(-1, keepdim=True)\n",
    "        # log probabilities\n",
    "        # domain range changed from [0, 1] -> [-inf, inf]\n",
    "        self.logits = probs_to_logits(self.probs)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        if not isinstance(sample_shape, torch.Size):\n",
    "            sample_shape = torch.Size(sample_shape)\n",
    "        # reshape the probs to 2D\n",
    "        probs_2d = self.probs.reshape(-1, self._num_events)\n",
    "        # for each row, return n results with replacement, n == 1 result in my case\n",
    "        samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T\n",
    "        # reshape the results to specified shape\n",
    "        return samples_2d.reshape(sample_shape + self._batch_shape + self._event_shape)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        value = value.long().unsqueeze(-1)\n",
    "        # make value and logits have matched shape\n",
    "        value, log_pmf = torch.broadcast_tensors(value, self.logits)\n",
    "        value = value[..., :1]\n",
    "        # for each row, return log_pmf[value[row]]\n",
    "        return log_pmf.gather(-1, value).squeeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        # to avoid large negative log probability when log(0) occurred\n",
    "        # we use \"eps\" instead of \"min\" here\n",
    "        min_real = torch.finfo(self.logits.dtype).min\n",
    "        logits = torch.clamp(self.logits, min=min_real)\n",
    "        # entropy\n",
    "        p_log_p = logits * self.probs\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "class PredictiveModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = state_dim + action_dim + hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.D = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim)\n",
    "        )\n",
    "        self.N = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim)\n",
    "        )\n",
    "        self.F = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, state, action, gru_out):\n",
    "        state = state.view(-1)\n",
    "        action = action.view(-1)\n",
    "        gru_out = gru_out.view(-1)\n",
    "        \n",
    "        x = torch.cat([state, action, gru_out], dim=-1)\n",
    "        delta = self.D(x) # D(s, a)\n",
    "        adjusted_state = state + delta # s + D(s, a)\n",
    "        \n",
    "        new_state = self.N(x) # N(s, a)\n",
    "        \n",
    "        forget_weights = self.F(x) # F(s, a), in [0,1]\n",
    "        \n",
    "        pred_s = forget_weights * adjusted_state + (1 - forget_weights) * new_state\n",
    "        return pred_s.view(-1, 1, self.out_dim)\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_seq_len, exps=None):\n",
    "        self.keys = [\n",
    "            \"states\", \"actions\", \"probs\", \"rewards\", \n",
    "            \"states_prime\", \"h_ins\", \"h_outs\", \"dones\", \n",
    "            \"timesteps\", \"pred_s_tis\", \"a_lsts\"\n",
    "        ]\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # used when copying other memory\n",
    "        self.init_exps() if exps is None else exps\n",
    "\n",
    "    def store(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self.exps:\n",
    "                self.exps[key].append(value)\n",
    "            else:\n",
    "                raise KeyError(f\"Invalid key '{key}' provided to store.\")\n",
    "\n",
    "    def init_exps(self):\n",
    "        self.exps = {key: [] for key in self.keys}\n",
    "\n",
    "    def get_current_size(self):\n",
    "        return len(next(iter(self.exps.values()), []))\n",
    "    \n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1   = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_pi = nn.Linear(hidden_dim, out_dim)\n",
    "        self.fc_v  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def pi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1   = nn.Linear(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, self.hidden_dim)\n",
    "        out, hidden = self.rnn(x, h)\n",
    "        return out, hidden\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, **config):\n",
    "        for key, value in config.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.rnn = RNN(self.s_size, 128, 64)\n",
    "        self.pred_model = PredictiveModel(self.s_size, 1, 64, self.s_size)\n",
    "        self.policy = Policy(64, 32, self.a_size)\n",
    "        self.memory = [Memory(self.T_horizon) for _ in range(self.num_memos)]\n",
    "        self.dist = Categorical((self.a_size,))\n",
    "        self.optim_pred_model = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.pred_model.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.optim_policy = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.policy.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "    \n",
    "    def sample_action(self, s, a_lst, h_in):\n",
    "        s = torch.from_numpy(s).float()\n",
    "        a_lst = torch.tensor(a_lst)\n",
    "        o, h_out, pred_s = self.pred_present(s, a_lst, h_in)\n",
    "        pi = self.policy.pi(o)\n",
    "        self.dist.set_probs(pi)\n",
    "        action = self.dist.sample()\n",
    "        return action.item(), pi, h_out, pred_s\n",
    "    \n",
    "    def pred_present(self, s, a_lst, h_in):\n",
    "        o_ti, h_first = self.rnn(s, h_in)\n",
    "        \n",
    "        s_ti = []\n",
    "        pred_s = s\n",
    "        h_ti = h_first\n",
    "        for i in range(self.p_iters):\n",
    "            pred_s = self.pred_model(pred_s, a_lst[i], o_ti)\n",
    "            s_ti.append(pred_s.view(-1))\n",
    "            o_ti, h_ti = self.rnn(pred_s, h_ti)\n",
    "        s_ti = torch.stack(s_ti) if len(s_ti) > 0 else torch.tensor([])\n",
    "            \n",
    "        return o_ti, h_first, s_ti\n",
    "    \n",
    "    def make_batch(self, i):\n",
    "        # retrieve from memory\n",
    "        s_lst, a_lst, prob_a_lst, r_lst, s_prime_lst, h_in_lst, h_out_lst, done_lst, _, _, a_lst_lst = \\\n",
    "            map(lambda key: self.memory[i].exps[key], self.memory[i].keys)\n",
    "        done_lst = [0 if done else 1 for done in done_lst]\n",
    "        \n",
    "        # reshape then return\n",
    "        s,a,r,s_prime,done_mask, prob_a = \\\n",
    "            torch.tensor(s_lst, dtype=torch.float).view(-1, self.s_size), torch.tensor(a_lst).view(-1, 1), \\\n",
    "            torch.tensor(r_lst).view(-1, 1), torch.tensor(s_prime_lst, dtype=torch.float).view(-1, self.s_size), \\\n",
    "            torch.tensor(done_lst, dtype=torch.float).view(-1, 1), torch.tensor(prob_a_lst).view(-1, 1)\n",
    "        a_lst = torch.tensor(a_lst_lst).view(-1, self.p_iters) if self.p_iters > 0 else torch.tensor([])\n",
    "        return s, a, r, s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0], a_lst\n",
    "\n",
    "    def _pi(self, s, h):\n",
    "        s = s.view(-1, 1, self.s_size)\n",
    "        x, h = self.rnn(s, h)\n",
    "        pi = self.policy.pi(x)\n",
    "        return pi.squeeze(1), h\n",
    "    \n",
    "    def _v(self, s, h):\n",
    "        s = s.view(-1, 1, self.s_size)\n",
    "        x, _ = self.rnn(s, h)\n",
    "        v = self.policy.v(x)\n",
    "        return v.squeeze(1)\n",
    "    \n",
    "    def cal_advantage(self, s, r, s_prime, done_mask, first_hidden, second_hidden):\n",
    "        v_prime = self._v(s_prime, second_hidden)\n",
    "        td_target = r + self.gamma * v_prime * done_mask\n",
    "        v_s = self._v(s, first_hidden)\n",
    "        delta = td_target - v_s\n",
    "        delta = delta.detach().numpy()\n",
    "\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "        return advantage, td_target\n",
    "        \n",
    "    def learn_policy(self, s,a,r,s_prime,done_mask, prob_a, first_hidden, second_hidden):\n",
    "        for i in range(self.K_epoch_policy):\n",
    "            advantage, td_target = self.cal_advantage(s, r, s_prime, done_mask, first_hidden, second_hidden)\n",
    "\n",
    "            pi, h = self._pi(s, first_hidden)\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self._v(s, first_hidden), td_target.detach())\n",
    "\n",
    "            self.optim_policy.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optim_policy.step()\n",
    "        \n",
    "    def make_pred_s_tis(self, s, a_lst, h_in, limit):\n",
    "        s_ti = []\n",
    "        for i in range(limit):\n",
    "            _, h_out, pred_s = self.pred_present(s[i], a_lst[i], h_in)\n",
    "            s_ti.append(pred_s)\n",
    "            h_in = h_out\n",
    "        return torch.stack(s_ti)\n",
    "        \n",
    "    def learn_pred_model(self):\n",
    "        if self.p_iters == 0: return\n",
    "        \n",
    "        loss_log = []\n",
    "        for _ in range(self.K_epoch_pred_model):\n",
    "            total_loss = 0\n",
    "            for i in range(self.num_memos):\n",
    "                s,a,r,s_prime,done_mask, prob_a, h_ins, h_outs, a_lst = self.make_batch(i)\n",
    "                (h_in, c_in), (h_out, c_out) = h_ins, h_outs\n",
    "                first_hidden  = (h_in.detach(), c_in.detach())\n",
    "                second_hidden = (h_out.detach(), c_out.detach())\n",
    "\n",
    "                target = []\n",
    "                limit = len(s) -self.p_iters\n",
    "                for i in range(limit):\n",
    "                    start, end = i + 1, min(i + self.p_iters, len(s) - 1) + 1\n",
    "                    before_done = s[start : end].tolist()\n",
    "                    after_done = [s[-1] for _ in range(self.p_iters - (end - start))]\n",
    "                    target.append(before_done + after_done)\n",
    "                target = torch.tensor(target, dtype=torch.float).view(-1, self.p_iters, self.s_size)\n",
    "\n",
    "                pred = self.make_pred_s_tis(s, a_lst, first_hidden, limit)\n",
    "                loss = self.pred_model.criterion(pred, target)\n",
    "                total_loss += loss\n",
    "            total_loss /= self.num_memos\n",
    "            \n",
    "            self.optim_pred_model.zero_grad()\n",
    "            total_loss.mean().backward()\n",
    "            self.optim_pred_model.step()\n",
    "            loss_log.append(total_loss)\n",
    "        print(f\"Loss: {torch.mean(torch.stack(loss_log))}\")\n",
    "            \n",
    "    def learn(self):\n",
    "        self.learn_pred_model()\n",
    "#         self.learn_policy(s,a,r,s_prime,done_mask, prob_a, first_hidden, second_hidden)\n",
    "        \n",
    "        for memo in self.memory:\n",
    "            memo.init_exps()\n",
    "            \n",
    "class Actor:\n",
    "    def __init__(self, **config):\n",
    "        for key, value in config.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "        self.rnn = RNN(self.s_size, 128, 64)\n",
    "        self.pred_model = PredictiveModel(self.s_size, 1, 64, self.s_size)\n",
    "        self.policy = Policy(64, 32, self.a_size)\n",
    "        self.optim_pred_model = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.pred_model.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.optim_policy = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.policy.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.dist = Categorical((self.a_size,))\n",
    "    \n",
    "    def sample_action(self, s, a_lst, h_in):\n",
    "        s = torch.from_numpy(s).float()\n",
    "        a_lst = torch.tensor(a_lst)\n",
    "        o, h_out, pred_s = self.pred_present(s, a_lst, h_in)\n",
    "        pi = self.policy.pi(o)\n",
    "        self.dist.set_probs(pi)\n",
    "        action = self.dist.sample()\n",
    "        return action.item(), pi, h_out, pred_s\n",
    "    \n",
    "    def pred_present(self, s, a_lst, h_in):\n",
    "        o_ti, h_first = self.rnn(s, h_in)\n",
    "        \n",
    "        s_ti = []\n",
    "        pred_s = s\n",
    "        h_ti = h_first\n",
    "        for i in range(self.p_iters):\n",
    "            pred_s = self.pred_model(pred_s, a_lst[i], o_ti)\n",
    "            s_ti.append(pred_s.view(-1))\n",
    "            o_ti, h_ti = self.rnn(pred_s, h_ti)\n",
    "        s_ti = torch.stack(s_ti) if len(s_ti) > 0 else torch.tensor([])\n",
    "            \n",
    "        return o_ti, h_first, s_ti\n",
    "    \n",
    "class Learner:\n",
    "    def __init__(self, **config):\n",
    "        for key, value in config.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "    \n",
    "    def make_batch(self, memory_list, i):\n",
    "        # retrieve from memory\n",
    "        s_lst, a_lst, prob_a_lst, r_lst, s_prime_lst, h_in_lst, h_out_lst, done_lst, _, _, a_lst_lst = \\\n",
    "            map(lambda key: memory_list[i].exps[key], memory_list[i].keys)\n",
    "        done_lst = [0 if done else 1 for done in done_lst]\n",
    "        \n",
    "        # reshape then return\n",
    "        s,a,r,s_prime,done_mask, prob_a = \\\n",
    "            torch.tensor(s_lst, dtype=torch.float).view(-1, self.s_size), torch.tensor(a_lst).view(-1, 1), \\\n",
    "            torch.tensor(r_lst).view(-1, 1), torch.tensor(s_prime_lst, dtype=torch.float).view(-1, self.s_size), \\\n",
    "            torch.tensor(done_lst, dtype=torch.float).view(-1, 1), torch.tensor(prob_a_lst).view(-1, 1)\n",
    "        a_lst = torch.tensor(a_lst_lst).view(-1, self.p_iters) if self.p_iters > 0 else torch.tensor([])\n",
    "        return s, a, r, s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0], a_lst\n",
    "    \n",
    "    def make_pred_s_tis(self, s, a_lst, h_in, limit):\n",
    "        s_ti = []\n",
    "        for i in range(limit):\n",
    "            _, h_out, pred_s = self.pred_present(s[i], a_lst[i], h_in)\n",
    "            s_ti.append(pred_s)\n",
    "            h_in = h_out\n",
    "        return torch.stack(s_ti)\n",
    "        \n",
    "    def learn_pred_model(self, memory_list):\n",
    "        if self.p_iters == 0: return\n",
    "        \n",
    "        loss_log = []\n",
    "        for _ in range(self.K_epoch_pred_model):\n",
    "            total_loss = 0\n",
    "            for i in range(self.num_memos):\n",
    "                s,a,r,s_prime,done_mask, prob_a, h_ins, h_outs, a_lst = self.make_batch(memory_list, i)\n",
    "                (h_in, c_in), (h_out, c_out) = h_ins, h_outs\n",
    "                first_hidden  = (h_in.detach(), c_in.detach())\n",
    "                second_hidden = (h_out.detach(), c_out.detach())\n",
    "\n",
    "                target = []\n",
    "                limit = len(s) -self.p_iters\n",
    "                for i in range(limit):\n",
    "                    start, end = i + 1, min(i + self.p_iters, len(s) - 1) + 1\n",
    "                    before_done = s[start : end].tolist()\n",
    "                    after_done = [s[-1] for _ in range(self.p_iters - (end - start))]\n",
    "                    target.append(before_done + after_done)\n",
    "                target = torch.tensor(target, dtype=torch.float).view(-1, self.p_iters, self.s_size)\n",
    "\n",
    "                pred = self.make_pred_s_tis(s, a_lst, first_hidden, limit)\n",
    "                loss = self.pred_model.criterion(pred, target)\n",
    "                total_loss += loss\n",
    "            total_loss /= self.num_memos\n",
    "            \n",
    "            self.optim_pred_model.zero_grad()\n",
    "            total_loss.mean().backward()\n",
    "            self.optim_pred_model.step()\n",
    "            loss_log.append(total_loss)\n",
    "        print(f\"Loss: {torch.mean(torch.stack(loss_log))}\")\n",
    "            \n",
    "    def learn(self):\n",
    "        self.learn_pred_model()\n",
    "#         self.learn_policy(s,a,r,s_prime,done_mask, prob_a, first_hidden, second_hidden)\n",
    "        \n",
    "#         for memo in self.memory:\n",
    "#             memo.init_exps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6be71b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([1])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "config = {\n",
    "    \"s_size\": gym.make(env_name).observation_space.shape[0],\n",
    "    \"a_size\": gym.make(env_name).action_space.n,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lmbda\": 0.95,\n",
    "    \"eps_clip\": 0.1,\n",
    "    \"K_epoch_policy\": 3,\n",
    "    \"K_epoch_pred_model\": 10,\n",
    "    \"delay\": 4,\n",
    "    \"p_iters\": 4,\n",
    "    \"num_memos\": 2,\n",
    "    \"T_horizon\": 500,\n",
    "    \"h0\": (torch.zeros([1, 1, 64], dtype=torch.float), torch.zeros([1, 1, 64], dtype=torch.float))\n",
    "}\n",
    "# env = gym.make(env_name)\n",
    "# actor = Actor(**config)\n",
    "# s = np.random.rand(5,4)\n",
    "# a_lst = [[i % 2 for i in range(actor.delay)] for _ in range(5)]\n",
    "# h_in = actor.h0\n",
    "# print(s)\n",
    "# print(a_lst)\n",
    "# a, prob, h_out, pred_s_ti = actor.sample_action(s, a_lst, h_in)\n",
    "# print(prob.shape)\n",
    "# print(h_out[0].shape)\n",
    "# print(pred_s_ti.shape)\n",
    "# env.close()\n",
    "a = torch.tensor([1, 2])\n",
    "print(a.shape)\n",
    "print(torch.tensor(1).view(-1).shape)\n",
    "b = torch.stack([a[0].view(-1), a[0].view(-1)])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795da9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env_name = \"CartPole-v1\"\n",
    "# env = gym.make(env_name)\n",
    "# config = {\n",
    "#     \"s_size\": env.observation_space.shape[0],\n",
    "#     \"a_size\": env.action_space.n,\n",
    "#     \"gamma\": 0.99,\n",
    "#     \"lmbda\": 0.95,\n",
    "#     \"eps_clip\": 0.1,\n",
    "#     \"K_epoch_policy\": 3,\n",
    "#     \"K_epoch_pred_model\": 10,\n",
    "#     \"delay\": 4,\n",
    "#     \"p_iters\": 4,\n",
    "#     \"num_memos\": 10,\n",
    "#     \"T_horizon\": 500\n",
    "# }\n",
    "\n",
    "# model = Agent(**config)\n",
    "# K_epoch_training = 100\n",
    "# score = 0.0\n",
    "# score_avg_interval = 20\n",
    "\n",
    "# print(\"Start.\")\n",
    "# for ep in range(1, K_epoch_training + 1):\n",
    "#     s, info = env.reset()\n",
    "#     h0 = torch.zeros([1, 1, 64], dtype=torch.float)\n",
    "#     h_out = (h0, h0)\n",
    "#     a_lst = [i % 2 for i in range(model.delay)]\n",
    "#     done = False\n",
    "    \n",
    "#     while not done:\n",
    "#         for t in range(model.T_horizon):\n",
    "#             h_in = h_out\n",
    "#             a, prob, h_out, pred_s_ti = model.sample_action(s, a_lst, h_in)\n",
    "#             prob = prob.view(-1)\n",
    "#             a_lst.append(a)\n",
    "\n",
    "#             delay_a = a_lst.pop(0)\n",
    "#             s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "#             done = terminated or truncated\n",
    "            \n",
    "#             exp = {\n",
    "#                 \"states\": s.tolist(),\n",
    "#                 \"actions\": delay_a,\n",
    "#                 \"probs\": prob[delay_a].item(),\n",
    "#                 \"rewards\": r / 100.0,\n",
    "#                 \"states_prime\": s_prime.tolist(),\n",
    "#                 \"h_ins\": h_in,\n",
    "#                 \"h_outs\": h_out,\n",
    "#                 \"dones\": done,\n",
    "#                 \"timesteps\": t,\n",
    "#                 \"pred_s_tis\": pred_s_ti,\n",
    "#                 \"a_lsts\": a_lst\n",
    "#             }\n",
    "#             model.memory[(ep - 1) % model.num_memos].store(**exp)\n",
    "#             s = s_prime\n",
    "#             score += r\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#     if ep % model.num_memos == 0:\n",
    "#         model.learn()\n",
    "        \n",
    "#     if ep % score_avg_interval == 0:\n",
    "#         print(f\"Ep. {ep - score_avg_interval + 1} ~ {ep}\", end=\", \")\n",
    "#         print(f\"avg score : {score / score_avg_interval:.1f}\")\n",
    "#         score = 0\n",
    "\n",
    "# env.close()\n",
    "# print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env_name, model, conn):\n",
    "    env = gym.make(env_name)\n",
    "    memory = Memory(model.T_horizon)\n",
    "    \n",
    "    s, info = env.reset()\n",
    "    h_out = model.h0\n",
    "    a_lst = [i % 2 for i in range(model.delay)]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        for t in range(model.T_horizon):\n",
    "            h_in = h_out\n",
    "            a, prob, h_out, pred_s_ti = model.sample_action(s, a_lst, h_in)\n",
    "            prob = prob.view(-1)\n",
    "            a_lst.append(a)\n",
    "\n",
    "            delay_a = a_lst.pop(0)\n",
    "            s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            exp = {\n",
    "                \"states\": s.tolist(),\n",
    "                \"actions\": delay_a,\n",
    "                \"probs\": prob[delay_a].item(),\n",
    "                \"rewards\": r / 100.0,\n",
    "                \"states_prime\": s_prime.tolist(),\n",
    "                \"h_ins\": h_in,\n",
    "                \"h_outs\": h_out,\n",
    "                \"dones\": done,\n",
    "                \"timesteps\": t,\n",
    "                \"pred_s_tis\": pred_s_ti,\n",
    "                \"a_lsts\": a_lst\n",
    "            }\n",
    "            memory.store(**exp)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "\n",
    "    conn.send(memory)\n",
    "    conn.close()\n",
    "    \n",
    "def parallel_process(env_name, model):\n",
    "    processes = []\n",
    "    parent_conns, child_conns = zip(*[mp.Pipe() for _ in range(model.num_memos)])\n",
    "\n",
    "    for i in range(model.num_memos):\n",
    "        p = mp.Process(target=collect_data, args=(env_name, model, child_conns[i]))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    aggregated_data = []\n",
    "    for conn in parent_conns:\n",
    "        data = conn.recv()\n",
    "        aggregated_data.append(data)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    env_name = \"CartPole-v1\"\n",
    "    config = {\n",
    "        \"s_size\": gym.make(env_name).observation_space.shape[0],\n",
    "        \"a_size\": gym.make(env_name).action_space.n,\n",
    "        \"gamma\": 0.99,\n",
    "        \"lmbda\": 0.95,\n",
    "        \"eps_clip\": 0.1,\n",
    "        \"K_epoch_policy\": 3,\n",
    "        \"K_epoch_pred_model\": 10,\n",
    "        \"delay\": 4,\n",
    "        \"p_iters\": 4,\n",
    "        \"num_memos\": 2,\n",
    "        \"T_horizon\": 500,\n",
    "        \"h0\": (torch.zeros([1, 1, 64], dtype=torch.float), torch.zeros([1, 1, 64], dtype=torch.float))\n",
    "    }\n",
    "\n",
    "    actor = Actor(**config)\n",
    "    learner = Learner(**config)\n",
    "    K_epoch_training = 1\n",
    "    score = 0.0\n",
    "    \n",
    "    print(\"Start.\")\n",
    "    for ep in range(1, K_epoch_training + 1):\n",
    "        memory_list = parallel_process(env_name, actor)\n",
    "        print(f\"Epoch. {ep}\", end=\", \")\n",
    "        print(f\"avg score : {score / actor.num_memos:.1f}\")\n",
    "        score = 0\n",
    "        print(len(memory_list))\n",
    "        # train p\n",
    "        # train policy\n",
    "\n",
    "    print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce984f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_eps = 10\n",
    "env = gym.make(env_name)\n",
    "total_score = []\n",
    "\n",
    "for ep in range(1, num_test_eps + 1):\n",
    "    s, info = env.reset()\n",
    "    h0 = torch.zeros([1, 1, 64], dtype=torch.float)\n",
    "    h_out = (h0, h0)\n",
    "    a_lst = [i % 2 for i in range(model.delay)]\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        h_in = h_out\n",
    "        a, prob, h_out, pred_s_ti = model.sample_action(s, a_lst, h_in)\n",
    "        prob = prob.view(-1)\n",
    "        a_lst.append(a)\n",
    "\n",
    "        delay_a = a_lst.pop(0)\n",
    "        s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        s = s_prime\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Ep. {ep}, score : {score}\")\n",
    "    total_score.append(score)\n",
    "\n",
    "env.close()\n",
    "print(\"Finished.\")\n",
    "print(f\"Average score : {sum(total_score) / len(total_score)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action_delay",
   "language": "python",
   "name": "action_delay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
