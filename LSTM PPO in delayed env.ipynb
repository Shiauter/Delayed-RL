{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b4f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.utils import probs_to_logits\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "learning_rate = 3e-4\n",
    "gamma         = 0.99\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 500\n",
    "\n",
    "class Categorical:\n",
    "    def __init__(self, probs_shape):\n",
    "        # NOTE: probs_shape is supposed to be\n",
    "        #       the shape of probs that will be\n",
    "        #       produced by policy network\n",
    "        if len(probs_shape) < 1:\n",
    "            raise ValueError(\"`probs_shape` must be at least 1.\")\n",
    "        self.probs_dim = len(probs_shape)\n",
    "        self.probs_shape = probs_shape\n",
    "        self._num_events = probs_shape[-1]\n",
    "        self._batch_shape = probs_shape[:-1] if self.probs_dim > 1 else torch.Size()\n",
    "        self._event_shape=torch.Size()\n",
    "\n",
    "    def set_probs(self, probs):\n",
    "        # normalized the probs\n",
    "        self.probs = probs / probs.sum(-1, keepdim=True)\n",
    "        # log probabilities\n",
    "        # domain range changed from [0, 1] -> [-inf, inf]\n",
    "        self.logits = probs_to_logits(self.probs)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        if not isinstance(sample_shape, torch.Size):\n",
    "            sample_shape = torch.Size(sample_shape)\n",
    "        # reshape the probs to 2D\n",
    "        probs_2d = self.probs.reshape(-1, self._num_events)\n",
    "        # for each row, return n results with replacement, n == 1 result in my case\n",
    "        samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T\n",
    "        # reshape the results to specified shape\n",
    "        return samples_2d.reshape(sample_shape + self._batch_shape + self._event_shape)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        value = value.long().unsqueeze(-1)\n",
    "        # make value and logits have matched shape\n",
    "        value, log_pmf = torch.broadcast_tensors(value, self.logits)\n",
    "        value = value[..., :1]\n",
    "        # for each row, return log_pmf[value[row]]\n",
    "        return log_pmf.gather(-1, value).squeeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        # to avoid large negative log probability when log(0) occurred\n",
    "        # we use \"eps\" instead of \"min\" here\n",
    "        min_real = torch.finfo(self.logits.dtype).min\n",
    "        logits = torch.clamp(self.logits, min=min_real)\n",
    "        # entropy\n",
    "        p_log_p = logits * self.probs\n",
    "        return -p_log_p.sum(-1)\n",
    "    \n",
    "class Memory:\n",
    "    def __init__(self, max_seq_len, exps=None):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if exps is None:\n",
    "            self.init_exps()\n",
    "        else:\n",
    "            self.exps = exps\n",
    "\n",
    "    def store(self, state, action, prob, reward, state_prime, h_in, h_out, done, timestep):\n",
    "        self.exps[\"states\"].append(state)\n",
    "        self.exps[\"actions\"].append(action)\n",
    "        self.exps[\"probs\"].append(prob)\n",
    "        self.exps[\"rewards\"].append(reward)\n",
    "        self.exps[\"states_prime\"].append(state_prime)\n",
    "        self.exps[\"h_ins\"].append(h_in)\n",
    "        self.exps[\"h_outs\"].append(h_out)\n",
    "        self.exps[\"dones\"].append(done)\n",
    "        self.exps[\"timesteps\"].append(timestep)\n",
    "\n",
    "    def init_exps(self):\n",
    "        self.exps = {\n",
    "            \"states\": [],\n",
    "            \"actions\": [], # sampled action at s_t\n",
    "            \"probs\": [],\n",
    "            \"rewards\": [], # immediate reward when leaving current s_t\n",
    "            \"states_prime\": [],\n",
    "            \"h_ins\": [],\n",
    "            \"h_outs\": [],\n",
    "            \"dones\": [],\n",
    "            \"timesteps\": []\n",
    "        }\n",
    "\n",
    "    def get_current_size(self):\n",
    "        return len(self.exps[\"states\"])\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.fc1   = nn.Linear(s_size,64)\n",
    "        self.lstm = nn.LSTM(64,32)\n",
    "        self.fc_pi = nn.Linear(32,a_size)\n",
    "        self.fc_v  = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob, lstm_hidden\n",
    "    \n",
    "    def v(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.policy = Policy()\n",
    "        self.memory = Memory(T_horizon)\n",
    "        self.dist = Categorical((a_size, ))\n",
    "    \n",
    "    def sample_action(self, s, h_in):\n",
    "        pi, h_out = self.policy.pi(torch.from_numpy(s).float(), h_in)\n",
    "        self.dist.set_probs(pi)\n",
    "        action = self.dist.sample()\n",
    "        return action.item(), pi, h_out\n",
    "    \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst, h_in_lst, h_out_lst = \\\n",
    "        self.memory.exps[\"states\"], self.memory.exps[\"actions\"], self.memory.exps[\"rewards\"], \\\n",
    "        self.memory.exps[\"states_prime\"], self.memory.exps[\"probs\"], self.memory.exps[\"dones\"], \\\n",
    "        self.memory.exps[\"h_ins\"], self.memory.exps[\"h_outs\"]\n",
    "            \n",
    "        done_lst = [0 if done else 1 for done in done_lst]            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float).view(-1, s_size), torch.tensor(a_lst).view(-1, 1), \\\n",
    "                                          torch.tensor(r_lst).view(-1, 1), torch.tensor(s_prime_lst, dtype=torch.float).view(-1, s_size), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float).view(-1, 1), torch.tensor(prob_a_lst).view(-1, 1)\n",
    "        return s, a, r, s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0]\n",
    "\n",
    "    def cal_advantage(self, s, r, s_prime, done_mask, first_hidden, second_hidden):\n",
    "        v_prime = self.policy.v(s_prime, second_hidden).squeeze(1)\n",
    "        td_target = r + gamma * v_prime * done_mask\n",
    "        v_s = self.policy.v(s, first_hidden).squeeze(1)\n",
    "        delta = td_target - v_s\n",
    "        delta = delta.detach().numpy()\n",
    "\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "        return advantage, td_target\n",
    "        \n",
    "    def learn(self):\n",
    "        s,a,r,s_prime,done_mask, prob_a, (h1_in, c1_in), (h2_out, c2_out) = self.make_batch()\n",
    "        first_hidden  = (h1_in.detach(), c1_in.detach())\n",
    "        second_hidden = (h2_out.detach(), c2_out.detach())\n",
    "        for i in range(K_epoch):\n",
    "            advantage, td_target = self.cal_advantage(s, r, s_prime, done_mask, first_hidden, second_hidden)\n",
    "\n",
    "            pi, _ = self.policy.pi(s, first_hidden)\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.policy.v(s, first_hidden).squeeze(1) , td_target.detach())\n",
    "\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.mean().backward(retain_graph=True)\n",
    "            self.policy.optimizer.step()\n",
    "        \n",
    "        self.memory.init_exps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7795da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 ~ 20, avg score : 19.2\n",
      "Ep. 21 ~ 40, avg score : 21.8\n",
      "Ep. 41 ~ 60, avg score : 22.4\n",
      "Ep. 61 ~ 80, avg score : 23.9\n",
      "Ep. 81 ~ 100, avg score : 35.8\n",
      "Ep. 101 ~ 120, avg score : 31.6\n",
      "Ep. 121 ~ 140, avg score : 46.4\n",
      "Ep. 141 ~ 160, avg score : 42.6\n",
      "Ep. 161 ~ 180, avg score : 52.1\n",
      "Ep. 181 ~ 200, avg score : 42.5\n",
      "Ep. 201 ~ 220, avg score : 34.8\n",
      "Ep. 221 ~ 240, avg score : 44.8\n",
      "Ep. 241 ~ 260, avg score : 44.8\n",
      "Ep. 261 ~ 280, avg score : 30.8\n",
      "Ep. 281 ~ 300, avg score : 36.1\n",
      "Ep. 301 ~ 320, avg score : 84.8\n",
      "Ep. 321 ~ 340, avg score : 66.4\n",
      "Ep. 341 ~ 360, avg score : 64.0\n",
      "Ep. 361 ~ 380, avg score : 58.2\n",
      "Ep. 381 ~ 400, avg score : 64.6\n",
      "Ep. 401 ~ 420, avg score : 73.2\n",
      "Ep. 421 ~ 440, avg score : 49.7\n",
      "Ep. 441 ~ 460, avg score : 65.2\n",
      "Ep. 461 ~ 480, avg score : 121.7\n",
      "Ep. 481 ~ 500, avg score : 86.0\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "num_eps = 500\n",
    "score_avg_interval = 20\n",
    "delay = 4\n",
    "\n",
    "env = gym.make(env_name)\n",
    "model = Agent()\n",
    "score = 0.0\n",
    "\n",
    "for ep in range(1, num_eps + 1):\n",
    "    s, info = env.reset()\n",
    "    h_out = (torch.zeros([1, 1, 32], dtype=torch.float), torch.zeros([1, 1, 32], dtype=torch.float)) # hidden_state, cell_state\n",
    "    action_queue = [env.action_space.sample() for _ in range(delay)]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            h_in = h_out\n",
    "            a, prob, h_out = model.sample_action(s, h_in)\n",
    "            prob = prob.view(-1)\n",
    "            action_queue.append(a)\n",
    "\n",
    "            delay_a = action_queue.pop(0)\n",
    "            s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "            done = terminated or truncated\n",
    "            model.memory.store(s, delay_a, prob[delay_a].item(), r/100.0, s_prime, h_in, h_out, done, t)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        model.learn()\n",
    "\n",
    "    if ep % score_avg_interval == 0:\n",
    "        print(f\"Ep. {ep - score_avg_interval + 1} ~ {ep}\", end=\", \")\n",
    "        print(f\"avg score : {score / score_avg_interval:.1f}\")\n",
    "        score = 0\n",
    "\n",
    "env.close()\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce984f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1, score : 85.0\n",
      "Ep. 2, score : 258.0\n",
      "Ep. 3, score : 86.0\n",
      "Ep. 4, score : 138.0\n",
      "Ep. 5, score : 68.0\n",
      "Ep. 6, score : 16.0\n",
      "Ep. 7, score : 10.0\n",
      "Ep. 8, score : 166.0\n",
      "Ep. 9, score : 180.0\n",
      "Ep. 10, score : 30.0\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "num_test_eps = 10\n",
    "env = gym.make(env_name)\n",
    "h_out = (torch.zeros([1, 1, 32], dtype=torch.float), torch.zeros([1, 1, 32], dtype=torch.float)) # hidden_state, cell_state\n",
    "\n",
    "for ep in range(1, num_test_eps + 1):\n",
    "    s, info = env.reset()\n",
    "    action_queue = [env.action_space.sample() for _ in range(delay)]\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        h_in = h_out\n",
    "        a, prob, h_out = model.sample_action(s, h_in)\n",
    "        prob = prob.view(-1)\n",
    "        action_queue.append(a)\n",
    "\n",
    "        delay_a = action_queue.pop(0)\n",
    "        s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "        done = terminated or truncated\n",
    "        s = s_prime\n",
    "        score += r\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Ep. {ep}, score : {score}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action_delay",
   "language": "python",
   "name": "action_delay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
