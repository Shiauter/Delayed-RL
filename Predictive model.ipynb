{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b4f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.utils import probs_to_logits\n",
    "import torch.multiprocessing as mp\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "gamma         = 0.99\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "T_horizon     = 500\n",
    "K_epoch_policy = 3\n",
    "K_epoch_pred_model = 10\n",
    "\n",
    "class Categorical:\n",
    "    def __init__(self, probs_shape):\n",
    "        # NOTE: probs_shape is supposed to be\n",
    "        #       the shape of probs that will be\n",
    "        #       produced by policy network\n",
    "        if len(probs_shape) < 1:\n",
    "            raise ValueError(\"`probs_shape` must be at least 1.\")\n",
    "        self.probs_dim = len(probs_shape)\n",
    "        self.probs_shape = probs_shape\n",
    "        self._num_events = probs_shape[-1]\n",
    "        self._batch_shape = probs_shape[:-1] if self.probs_dim > 1 else torch.Size()\n",
    "        self._event_shape=torch.Size()\n",
    "\n",
    "    def set_probs(self, probs):\n",
    "        # normalized the probs\n",
    "        self.probs = probs / probs.sum(-1, keepdim=True)\n",
    "        # log probabilities\n",
    "        # domain range changed from [0, 1] -> [-inf, inf]\n",
    "        self.logits = probs_to_logits(self.probs)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        if not isinstance(sample_shape, torch.Size):\n",
    "            sample_shape = torch.Size(sample_shape)\n",
    "        # reshape the probs to 2D\n",
    "        probs_2d = self.probs.reshape(-1, self._num_events)\n",
    "        # for each row, return n results with replacement, n == 1 result in my case\n",
    "        samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T\n",
    "        # reshape the results to specified shape\n",
    "        return samples_2d.reshape(sample_shape + self._batch_shape + self._event_shape)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        value = value.long().unsqueeze(-1)\n",
    "        # make value and logits have matched shape\n",
    "        value, log_pmf = torch.broadcast_tensors(value, self.logits)\n",
    "        value = value[..., :1]\n",
    "        # for each row, return log_pmf[value[row]]\n",
    "        return log_pmf.gather(-1, value).squeeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        # to avoid large negative log probability when log(0) occurred\n",
    "        # we use \"eps\" instead of \"min\" here\n",
    "        min_real = torch.finfo(self.logits.dtype).min\n",
    "        logits = torch.clamp(self.logits, min=min_real)\n",
    "        # entropy\n",
    "        p_log_p = logits * self.probs\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "class PredictiveModel(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        state_dim = s_size\n",
    "        action_dim = 1\n",
    "        self.in_dim = state_dim + action_dim + hidden_dim\n",
    "        self.out_dim = state_dim\n",
    "        \n",
    "        self.D = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim)\n",
    "        )\n",
    "        self.N = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim)\n",
    "        )\n",
    "        self.F = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, state, action, gru_out):\n",
    "        state = state.view(-1)\n",
    "        action = action.view(-1)\n",
    "        gru_out = gru_out.view(-1)\n",
    "        \n",
    "        x = torch.cat([state, action, gru_out], dim=-1)\n",
    "        delta = self.D(x) # D(s, a)\n",
    "        adjusted_state = state + delta # s + D(s, a)\n",
    "        \n",
    "        new_state = self.N(x) # N(s, a)\n",
    "        \n",
    "        forget_weights = self.F(x) # F(s, a), in [0,1]\n",
    "        \n",
    "        pred_s = forget_weights * adjusted_state + (1 - forget_weights) * new_state\n",
    "        return pred_s.view(-1, 1, s_size)\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_seq_len, exps=None):\n",
    "        self.keys = [\n",
    "            \"states\", \"actions\", \"probs\", \"rewards\", \n",
    "            \"states_prime\", \"h_ins\", \"h_outs\", \"dones\", \n",
    "            \"timesteps\", \"pred_s_tis\", \"a_lsts\"\n",
    "        ]\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # used when copying other memory\n",
    "        self.init_exps() if exps is None else exps\n",
    "\n",
    "    def store(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self.exps:\n",
    "                self.exps[key].append(value)\n",
    "            else:\n",
    "                raise KeyError(f\"Invalid key '{key}' provided to store.\")\n",
    "\n",
    "    def init_exps(self):\n",
    "        self.exps = {key: [] for key in self.keys}\n",
    "\n",
    "    def get_current_size(self):\n",
    "        return len(next(iter(self.exps.values()), []))\n",
    "    \n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1   = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_pi = nn.Linear(hidden_dim, a_size)\n",
    "        self.fc_v  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def pi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1   = nn.Linear(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, self.hidden_dim)\n",
    "        out, hidden = self.rnn(x, h)\n",
    "        return out, hidden\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, p_iters=0, num_memos=10):\n",
    "        self.p_iters = p_iters\n",
    "        self.num_memos = num_memos\n",
    "        \n",
    "        self.rnn = RNN(s_size, 128, 64)\n",
    "        self.pred_model = PredictiveModel(64)\n",
    "        self.policy = Policy(64, 32)\n",
    "        self.memory = [Memory(T_horizon) for _ in range(num_memos)]\n",
    "        self.dist = Categorical((a_size, ))\n",
    "        self.optim_pred_model = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.pred_model.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.optim_policy = optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.rnn.parameters()}, \n",
    "                {\"params\": self.policy.parameters()}\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "    \n",
    "    def sample_action(self, s, a_lst, h_in):\n",
    "        s = torch.from_numpy(s).float()\n",
    "        a_lst = torch.tensor(a_lst)\n",
    "        o, h_out, pred_s = self.pred_present(s, a_lst, h_in)\n",
    "        pi = self.policy.pi(o)\n",
    "        self.dist.set_probs(pi)\n",
    "        action = self.dist.sample()\n",
    "        return action.item(), pi, h_out, pred_s\n",
    "    \n",
    "    def pred_present(self, s, a_lst, h_in):\n",
    "        o_ti, h_first = self.rnn(s, h_in)\n",
    "        \n",
    "        s_ti = []\n",
    "        pred_s = s\n",
    "        h_ti = h_first\n",
    "        for i in range(self.p_iters):\n",
    "            pred_s = self.pred_model(pred_s, a_lst[i], o_ti)\n",
    "            s_ti.append(pred_s.view(-1))\n",
    "            o_ti, h_ti = self.rnn(pred_s, h_ti)\n",
    "        s_ti = torch.stack(s_ti) if len(s_ti) > 0 else torch.tensor([])\n",
    "            \n",
    "        return o_ti, h_first, s_ti\n",
    "    \n",
    "    def make_batch(self, i):\n",
    "        # retrieve from memory\n",
    "        s_lst, a_lst, prob_a_lst, r_lst, s_prime_lst, h_in_lst, h_out_lst, done_lst, _, _, a_lst_lst = \\\n",
    "            map(lambda key: self.memory[i].exps[key], self.memory[i].keys)\n",
    "        done_lst = [0 if done else 1 for done in done_lst]\n",
    "        \n",
    "        # reshape then return\n",
    "        s,a,r,s_prime,done_mask, prob_a = \\\n",
    "            torch.tensor(s_lst, dtype=torch.float).view(-1, s_size), torch.tensor(a_lst).view(-1, 1), \\\n",
    "            torch.tensor(r_lst).view(-1, 1), torch.tensor(s_prime_lst, dtype=torch.float).view(-1, s_size), \\\n",
    "            torch.tensor(done_lst, dtype=torch.float).view(-1, 1), torch.tensor(prob_a_lst).view(-1, 1)\n",
    "        a_lst = torch.tensor(a_lst_lst).view(-1, self.p_iters) if self.p_iters > 0 else torch.tensor([])\n",
    "        return s, a, r, s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0], a_lst\n",
    "\n",
    "    def _pi(self, s, h):\n",
    "        s = s.view(-1, 1, s_size)\n",
    "        x, h = self.rnn(s, h)\n",
    "        pi = self.policy.pi(x)\n",
    "        return pi.squeeze(1), h\n",
    "    \n",
    "    def _v(self, s, h):\n",
    "        s = s.view(-1, 1, s_size)\n",
    "        x, _ = self.rnn(s, h)\n",
    "        v = self.policy.v(x)\n",
    "        return v.squeeze(1)\n",
    "    \n",
    "    def cal_advantage(self, s, r, s_prime, done_mask, first_hidden, second_hidden):\n",
    "        v_prime = self._v(s_prime, second_hidden)\n",
    "        td_target = r + gamma * v_prime * done_mask\n",
    "        v_s = self._v(s, first_hidden)\n",
    "        delta = td_target - v_s\n",
    "        delta = delta.detach().numpy()\n",
    "\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "        return advantage, td_target\n",
    "        \n",
    "    def learn_policy(self, s,a,r,s_prime,done_mask, prob_a, first_hidden, second_hidden):\n",
    "        for i in range(K_epoch_policy):\n",
    "            advantage, td_target = self.cal_advantage(s, r, s_prime, done_mask, first_hidden, second_hidden)\n",
    "\n",
    "            pi, h = self._pi(s, first_hidden)\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self._v(s, first_hidden), td_target.detach())\n",
    "\n",
    "            self.optim_policy.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optim_policy.step()\n",
    "        \n",
    "    def make_pred_s_tis(self, s, a_lst, h_in, limit):\n",
    "        s_ti = []\n",
    "        for i in range(limit):\n",
    "            _, h_out, pred_s = self.pred_present(s[i], a_lst[i], h_in)\n",
    "            s_ti.append(pred_s)\n",
    "            h_in = h_out\n",
    "        return torch.stack(s_ti)\n",
    "        \n",
    "    def learn_pred_model(self):\n",
    "        if self.p_iters == 0: return\n",
    "        \n",
    "        loss_log = []\n",
    "        for _ in range(K_epoch_pred_model):\n",
    "            total_loss = 0\n",
    "            for i in range(self.num_memos):\n",
    "                s,a,r,s_prime,done_mask, prob_a, h_ins, h_outs, a_lst = self.make_batch(i)\n",
    "                (h_in, c_in), (h_out, c_out) = h_ins, h_outs\n",
    "                first_hidden  = (h_in.detach(), c_in.detach())\n",
    "                second_hidden = (h_out.detach(), c_out.detach())\n",
    "\n",
    "                target = []\n",
    "                limit = len(s) -self.p_iters\n",
    "                for i in range(limit):\n",
    "                    start, end = i + 1, min(i + self.p_iters, len(s) - 1) + 1\n",
    "                    before_done = s[start : end].tolist()\n",
    "                    after_done = [s[-1] for _ in range(self.p_iters - (end - start))]\n",
    "                    target.append(before_done + after_done)\n",
    "                target = torch.tensor(target, dtype=torch.float).view(-1, self.p_iters, s_size)\n",
    "\n",
    "                pred = self.make_pred_s_tis(s, a_lst, first_hidden, limit)\n",
    "                loss = self.pred_model.criterion(pred, target)\n",
    "                total_loss += loss\n",
    "            total_loss /= self.num_memos\n",
    "            \n",
    "            self.optim_pred_model.zero_grad()\n",
    "            total_loss.mean().backward()\n",
    "            self.optim_pred_model.step()\n",
    "            loss_log.append(total_loss)\n",
    "        print(f\"Loss: {torch.mean(torch.stack(loss_log))}\")\n",
    "        \n",
    "#         s_lst = self.memory.exps[\"states\"]\n",
    "#         target = []\n",
    "#         for i in range(len(s_lst)):\n",
    "#             start, end = i + 1, min(i + self.p_iters, len(s_lst) - 1) + 1\n",
    "#             after_done = [s_lst[-1] for _ in range(self.p_iters - (end - start))]\n",
    "#             target.append(s_lst[start : end] + after_done)\n",
    "#         target = torch.tensor(target, dtype=torch.float).view(-1, self.p_iters, s_size)\n",
    "        \n",
    "#         mean_loss = 0\n",
    "#         # to-do \n",
    "#         for _ in range(K_epoch_pred_model):\n",
    "#             pred = self.make_pred_s_tis(s, a_lst, h_in)\n",
    "#             loss = self.pred_model.criterion(pred, target)\n",
    "        \n",
    "#             self.optim_pred_model.zero_grad()\n",
    "#             loss.mean().backward()\n",
    "#             self.optim_pred_model.step()\n",
    "#             mean_loss += loss.item()\n",
    "            \n",
    "#         print(mean_loss / K_epoch_pred_model)\n",
    "            \n",
    "    def learn(self):\n",
    "        self.learn_pred_model()\n",
    "#         self.learn_policy(s,a,r,s_prime,done_mask, prob_a, first_hidden, second_hidden)\n",
    "        \n",
    "        for memo in self.memory:\n",
    "            memo.init_exps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7795da9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.20449385046958923\n",
      "Loss: 0.1440919190645218\n",
      "Ep. 1 ~ 20, avg score : 25.2\n",
      "Loss: 0.13292978703975677\n",
      "Loss: 0.08256419003009796\n",
      "Ep. 21 ~ 40, avg score : 30.8\n",
      "Loss: 0.06558500230312347\n",
      "Loss: 0.07339993119239807\n",
      "Ep. 41 ~ 60, avg score : 29.5\n",
      "Loss: 0.08932764083147049\n",
      "Loss: 0.06470133364200592\n",
      "Ep. 61 ~ 80, avg score : 24.5\n",
      "Loss: 0.06744922697544098\n",
      "Loss: 0.07532991468906403\n",
      "Ep. 81 ~ 100, avg score : 24.1\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "score_avg_interval = 20\n",
    "delay = 4\n",
    "\n",
    "env = gym.make(env_name)\n",
    "model = Agent(p_iters=delay)\n",
    "score = 0.0\n",
    "num_eps = 10 * model.num_memos\n",
    "\n",
    "for ep in range(1, num_eps + 1):\n",
    "    s, info = env.reset()\n",
    "    h0 = torch.zeros([1, 1, 64], dtype=torch.float)\n",
    "    h_out = (h0, h0)\n",
    "    a_lst = [i % 2 for i in range(delay)]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            h_in = h_out\n",
    "            a, prob, h_out, pred_s_ti = model.sample_action(s, a_lst, h_in)\n",
    "            prob = prob.view(-1)\n",
    "            a_lst.append(a)\n",
    "\n",
    "            delay_a = a_lst.pop(0)\n",
    "            s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            exp = {\n",
    "                \"states\": s,\n",
    "                \"actions\": delay_a,\n",
    "                \"probs\": prob[delay_a].item(),\n",
    "                \"rewards\": r / 100.0,\n",
    "                \"states_prime\": s_prime,\n",
    "                \"h_ins\": h_in,\n",
    "                \"h_outs\": h_out,\n",
    "                \"dones\": done,\n",
    "                \"timesteps\": t,\n",
    "                \"pred_s_tis\": pred_s_ti,\n",
    "                \"a_lsts\": a_lst\n",
    "            }\n",
    "            model.memory[(ep - 1) % model.num_memos].store(**exp)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    if ep % model.num_memos == 0:\n",
    "        model.learn()\n",
    "        \n",
    "    if ep % score_avg_interval == 0:\n",
    "        print(f\"Ep. {ep - score_avg_interval + 1} ~ {ep}\", end=\", \")\n",
    "        print(f\"avg score : {score / score_avg_interval:.1f}\")\n",
    "        score = 0\n",
    "\n",
    "env.close()\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce984f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 1, 64), got [1, 1, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     14\u001b[0m     h_in \u001b[38;5;241m=\u001b[39m h_out\n\u001b[1;32m---> 15\u001b[0m     a, prob, h_out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     prob \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m     a_lst\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "Cell \u001b[1;32mIn[33], line 197\u001b[0m, in \u001b[0;36mAgent.sample_action\u001b[1;34m(self, s, a_lst, h_in)\u001b[0m\n\u001b[0;32m    195\u001b[0m s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    196\u001b[0m a_lst \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(a_lst)\n\u001b[1;32m--> 197\u001b[0m o, h_out, pred_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_present\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpi(o)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39mset_probs(pi)\n",
      "Cell \u001b[1;32mIn[33], line 204\u001b[0m, in \u001b[0;36mAgent.pred_present\u001b[1;34m(self, s, a_lst, h_in)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpred_present\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a_lst, h_in):\n\u001b[1;32m--> 204\u001b[0m     o_ti, h_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     s_ti \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    207\u001b[0m     pred_s \u001b[38;5;241m=\u001b[39m s\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 166\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x, h)\u001b[0m\n\u001b[0;32m    164\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m    165\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[1;32m--> 166\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, hidden\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:913\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    910\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m--> 913\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:828\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    826\u001b[0m                        ):\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\action_delay\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:266\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    264\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 64), got [1, 1, 32]"
     ]
    }
   ],
   "source": [
    "num_test_eps = 10\n",
    "env = gym.make(env_name)\n",
    "h0 = torch.zeros([1, 1, 32], dtype=torch.float)\n",
    "h_out = (h0, h0)\n",
    "total_score = []\n",
    "\n",
    "for ep in range(1, num_test_eps + 1):\n",
    "    s, info = env.reset()\n",
    "    a_lst = [env.action_space.sample() for _ in range(delay)]\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        h_in = h_out\n",
    "        a, prob, h_out, _ = model.sample_action(s, a_lst, h_in)\n",
    "        prob = prob.view(-1)\n",
    "        a_lst.append(a)\n",
    "\n",
    "        delay_a = a_lst.pop(0)\n",
    "        s_prime, r, terminated, truncated, info = env.step(delay_a)\n",
    "        done = terminated or truncated\n",
    "        s = s_prime\n",
    "        score += r\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Ep. {ep}, score : {score}\")\n",
    "    total_score.append(score)\n",
    "\n",
    "env.close()\n",
    "print(\"Finished.\")\n",
    "print(f\"Average score : {sum(total_score) / len(total_score)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action_delay",
   "language": "python",
   "name": "action_delay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
